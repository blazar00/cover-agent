[mutation_test_prompt]
system="""\
"""

user="""\
You are an AI mutation testing agent. Your task: mutate {{ language }} code to test robustness. Use the provided Abstract Syntax Tree (AST) for context. Read the AST before mutating.

Mutation Guidelines:
1. Logic Modification:
   - Alter conditions: e.g., 'if (a < b)' to 'if (a <= b)'
   - Change loop boundaries
   - Introduce calculation errors
   Avoid: Infinite loops, excessive logic changes

2. Output Alteration:
   - Change return types
   - Modify response structures
   - Return corrupted data

3. Method Call Changes:
   - Tamper with parameters
   - Replace or remove critical functions

4. Failure Simulation:
   - Inject exceptions
   - Simulate resource failures

5. Data Handling Errors:
   - Introduce parsing errors
   - Bypass data validation
   - Alter object states incorrectly

6. Boundary Testing:
   - Use out-of-bounds indices
   - Test with extreme parameter values

7. Concurrency Issues:
   - Create race conditions
   - Introduce potential deadlocks
   - Simulate timeouts

8. Security Vulnerabilities:
   - Replicate common CVE bugs (e.g., buffer overflow, SQL injection, XSS)
   - Introduce authentication bypasses

Apply mutations strategically. Focus on subtle changes that test code resilience without breaking core functionality. Aim for realistic scenarios that could occur due to programming errors or edge cases.


## Source Code to Mutate: {{ source_file_name }}
```{{language}}
{{ source_file_numbered }}
```

## Task
1. Analyze the source code line by line.
2. Generate mutations for each test
3. Focus on function blocks and critical areas.
4. Ensure mutations provide insights into code quality and test coverage.
5. Organize output by ascending line numbers.
6. Do not include manually added line numbers in your response.
7. Generate single-line mutations only.

## Response
The output must be a YAML object equivalent to type $NewTests, according to the following Pydantic definitions:
=====
class SingleTest(BaseModel):
    test_behavior: str = Field(description="Short description of the behavior the test covers")
{%- if language in ["python","java"] %}
    lines_to_cover: str = Field(description="A list of line numbers, currently uncovered, that this specific new test aims to cover")
    test_name: str = Field(description=" A short test name, in snake case, that reflects the behaviour to test")
{%- else %}
    test_name: str = Field(description=" A short unique test name, that should reflect the test objective")
{%- endif %}
    test_code: str = Field(description="A single test function, that tests the behavior described in 'test_behavior'. The test should be a written like its a part of the existing test suite, if there is one, and it can use existing helper functions, setup, or teardown code.")
    new_imports_code: str = Field(description="New imports that are required to run the new test function, and are not already imported in the test file. Give an empty string if no new imports are required. If relevant, add new imports as  'import ...' lines.")
    test_tags: str = Field(description="A single label that best describes the test, out of: ['happy path', 'edge case','other']")

class NewTests(BaseModel):
    language: str = Field(description="The programming language of the source code")
    existing_test_function_signature: str = Field(description="A single line repeating a signature header of one of the existing test functions")
    new_tests: List[SingleTest] = Field(min_items=1, max_items={{ max_tests }}, description="A list of new test functions to append to the existing test suite, aiming to increase the code coverage. Each test should run as-is, without requiring any additional inputs or setup code. Don't introduce new dependencies")
=====

Example output:
```yaml
language: {{ language }}
existing_test_function_signature: |
  ...
new_tests:
- test_behavior: |
    Test that the function returns the correct output for a single element list
{%- if language in ["python","java"] %}
  lines_to_cover: |
    [1,2,5, ...]
  test_name: |
    test_single_element_list
{%- else %}
  test_name: |
    ...
{%- endif %}
  test_code: |
{%- if language in ["python"] %}
    def ...
{%- else %}
    ...
{%- endif %}
  new_imports_code: |
    ""
  test_tags: happy path
    ...
```

Use block scalar('|') to format each YAML output.

Response (should be a valid YAML, and nothing else):
```yaml

Produce mutants that challenge the robustness of the code without breaking core functionality. Provide only the YAML output. Do not include any additional explanations or comments.
"""